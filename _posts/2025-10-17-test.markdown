---
layout:     post
title:      "TEST"
subtitle:   "this is test for blong"
date:       2025.10.17
author:     "Zihan Zhang"
catalog: false
published: false
header-style: text
tags:
  - test
---
# 2025.10-1

## 2025.10.3

## 2025.10.4

## 2025.10.5

周日休息

## 2025.10.6

## 2025.10.7

好久都没做学习笔记了，因为前段时间放假和室友出去玩了一天，加上礼拜日休息，一直没有连贯地学习，今天记录一下。

这几天一直在看SNN，因为我现在的科研思路就是用Flink去跑机器学习嘛，然后传统的ANN面向的还是批处理，但是SNN的特性更偏向流处理。且SNN中神经元的膜电位概念天然地使得SNN具有时许性。再加上ANN现在已经有比较成熟地分布式处理框架了，而SNN在这一方面研究的还比较少。综上所述，我的现在选择SNN。

科研目标是这样变化的：

Flink分布式地使用GPU资源。考虑到GPU当前使用的活跃领域是机器学习的计算 👉 Flink分布式进行神经网络训练。考虑到SNN具有时序性、连续性的特点 👉 Flink分布式进行SNN训练。

可以看出是一个逐渐收窄或者说逐渐明确的过程。

然后我这两天自己搭建了一个SNN框架，SNN使用LIF模型。因为SNN不能用梯度下降，所以我提出并研究了一种名为“情感驱动”的监督学习方法。核心思路就是赫布学习法，一旦输出神经元被激活，如果符合tag，那么被激活的神经元与激活它的上游神经元之间的通路被加强，一直向上追溯直至输出层。反之则削弱通路。

数学表达大概是：
$$
\text{设定一个感性值}S \in (0,+\infty)\text{和情绪函数}e(output,tag) \in (-1,1)\\
\text{对第 i 层神经网络的激活向量}v_{i},v_{i}\text{中为1的分量表示该层对应位置的神经元被激活，0表示未被激活。}\\
\text{当网络进行输出后，第 i 层与 i+1 层之前的连接权重更新为：}W_i \gets W_i + S \cdot e(output,tag) \cdot v_{i} \cdot v_{i+1}^{\top}
$$
我进行了实验，用一个5-2 SNN进行5以内的数字进行奇偶分类。这个任务抽象成数学问题就是
$$
\text{Give an one-hot vector } V_{input} \in \{0,1\}^5 \ and\ \sum_{i=1}^5{x_i}=1 \\
\text{there is a mapping } SNN(V_{input},W,V_{mp}) \in \{0,1\}^2,V_{mp}\text{ is membrane voltage of SNN}\\
\text{Find a matrix } W_{object} \text{ make maximize }p(SNN=
\begin{bmatrix}
1 & 0 & 1 & 0 & 1 \\
0 & 1 & 0 & 1 & 0
\end{bmatrix}
\cdot V_{input} \ |\ W = W_{object}
)
$$
这是一个批处理任务，确实不太适合SNN，但是我主要是想验证这种情绪驱动的学习方式。这时我发现这种学习方式会出现一个问题。如果初始生成的权重矩阵一行内两个分量太过相近，那么输出神经元将会被同时激活，进而同时削弱或增强。虽然理论上只要不完全相同，总有异步激活时候然后加强对应的神经通路，但是这样训练时间太久，于是我给每次前向传播时给权重每个分量都增加了一个小扰动，这样可以加快通路的分流。

我加了一层隐层后形成了一个5-8-2网络，想要进行32以内的数字的奇偶分类，就会一直出现问题。网络一直难以收敛。我试了很多方法，比如情绪分区、学习率下降等，都没有实质性的用处。因而在这个问题上我决定直接继承前人的学术成果，这一次的摸索就是一个实践过程。

SNN当前研究和资料很少，但是我找到了一个SNN框架，是北大田永鸿教授团队的SpikingJelly(惊蜇)框架，等有空研究一下吧。

奥对了，还有一个有意思的东西，受脑电波的启发，我可以把SNN中所有被激活的神经元的个数依时许汇成一个波形图，这个可以被视为SNN脑电波，我觉得这个东西也许对于大规模SNN的研究会有一些帮助。

## 2025.10.8

SNN现在不是没有一个很合适的训练方式吗，这个找训练方式的问题可以抽象为下面这个数学问题：
$$
\text{Find a function }train \text{ such that }T = \min \Big\{ t \in \mathbb{N}\ \big|\ \mathbb{P}(o_t = \hat o_t \mid train, I, \hat O,W_0) \ge p \Big\},\\
\begin{array}{l}
\text{Where:} \\
p \in (0,1) \text{ is a probability threshold},\\
\hat o_t \text{ denotes the target output}.\\
N = \{0,1,\dots,m-1\}, \quad N_\text{in} \subset N, \quad N_\text{out} \subset N, \quad N_\text{hid} = N \setminus (N_\text{in} \cup N_\text{out})\\
t = 0,1,\dots,T-1, \quad \text{time index} \\
v_t \in \{0,1\}^m, \ \text{binary spike vector of all neurons at time } t \\
mp_t \in [0,1)^m, \ \text{membrane potential vector at time } t \\
i_t \in \{0,1\}^{|N_\text{in}|}, \quad I = \{i_t\}_{t=0}^{T-1}, \ \text{input matrix, zero-padded to length } m \\
o_t \in \{0,1\}^{|N_\text{out}|}, \quad O = \{o_t\}_{t=0}^{T-1}, \ \text{output matrix, zero-padded to length } m \\
W \in [0,1)^{m \times m}, \quad w_{i,j} \text{ is the synaptic weight from neuron } i \text{ to neuron } j \\
ON_j =
\begin{cases}
1, & j \in N_\text{out} \\
0, & j \notin N_\text{out}
\end{cases}, \ \text{output mask vector of length } m \\
R(mp)_j = r(mp_j), \quad r(x) =
\begin{cases}
0, & x \ge V_\text{th} \\
x, & x < V_\text{th}
\end{cases}, \ \text{reset function applied element-wise} \\
S(mp)_j = s(mp_j), \quad s(x) =
\begin{cases}
1, & x \ge V_\text{th} \\
0, & x < V_\text{th}
\end{cases}, \ \text{spike function applied element-wise} \\
\text{SNN equations:} \\
\quad
\begin{cases}
(1)\ v_t = i_t \lor sp_{t-1}, \quad \text{logical OR to avoid values > 1} \\
(2)\ mp_t = W v_t, \quad \text{linear accumulation of membrane potential} \\
(3)\ sp_t = S(mp_t), \quad \text{spike emission} \\
(4)\ mp_{t+1} = (1-\lambda )R(mp_t), \quad \text{membrane voltage reset method of LIF model} \\
(5)\ W_{t+1} = \text{train}(W_t, \dots), \quad \text{weight update} \\
(6)\ o_t = sp_t \circ ON, \quad \text{apply output mask} \\
\end{cases} \\
\end{array}
$$
所以神经网络SNN的最优训练方法只需要解出方程$$T = \min \Big\{ t \in \mathbb{N}\ \big|\ \mathbb{P}(o_t = \hat o_t \mid train, I, \hat O,W_0) \ge p \Big\}$$就行了。感觉需要一点泛函的知识。。。

刚才查了查，这应该是一个泛函优化问题，，哈哈，果然不是我这个阶段能做的课题。奥，这类问题叫变分问题，和当年牛顿求最大降速曲线的问题本质上是一样的

## 2025.10.9

___

## 总结

半工半休过了一礼拜，也没啥可记的。

这个礼拜最大的收获就是研究了一下神经网络训练步变分方程，我让GPT给我扩展成一般情况并尝试求解，他给我写了篇论文：（typero显示不出来，我另存pdf）
